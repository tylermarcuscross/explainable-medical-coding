@article{Dirac1953888,
  title   = {The {Lorentz} transformation and absolute time},
  journal = {Physica },
  volume  = {19},
  number  = {1-–12},
  pages   = {888--896},
  year    = {1953},
  doi     = {10.1016/S0031-8914(53)80099-6},
  author  = {P. A. M. Dirac}
}

@article{Feynman1963118,
  title   = {The theory of a general quantum system interacting with a linear dissipative system},
  journal = {Annals of Physics },
  volume  = {24},
  pages   = {118--173},
  year    = {1963},
  doi     = {10.1016/0003-4916(63)90068-X},
  author  = {R. P Feynman and F. L {Vernon Jr.}}
}

@misc{warner2024modernbert,
      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, 
      author={Benjamin Warner and Antoine Chaffin and Benjamin Clavié and Orion Weller and Oskar Hallström and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},
      year={2024},
      eprint={2412.13663},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.13663}
}

@misc{edin2024explainable,
  title = {An {Unsupervised} {Approach} to {Achieve} {Supervised-Level} {Explainability} in {Healthcare} {Records}},
  author = {Edin, Joakim and Maistro, Maria and Maal{\o}e, Lars and Borgholt, Lasse and Havtorn, Jakob D. and Ruotsalo, Tuukka},
  year = {2024},
  month = jun,
  number = {arXiv:2406.08958},
  eprint = {2406.08958},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-14},
  abstract = {Electronic healthcare records are vital for patient safety as they document conditions, plans, and procedures in both free text and medical codes. Language models have significantly enhanced the processing of such records, streamlining workflows and reducing manual data entry, thereby saving healthcare providers significant resources. However, the black-box nature of these models often leaves healthcare professionals hesitant to trust them. State-of-the-art explainability methods increase model transparency but rely on human-annotated evidence spans, which are costly. In this study, we propose an approach to produce plausible and faithful explanations without needing such annotations. We demonstrate on the automated medical coding task that adversarial robustness training improves explanation plausibility and introduce AttInGrad, a new explanation method superior to previous ones. By combining both contributions in a fully unsupervised setup, we produce explanations of comparable quality, or better, to that of a supervised approach. We release our code and model weights.},
  archiveprefix = {arxiv}
}